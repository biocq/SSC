% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CoBC.R
\name{coBCBase}
\alias{coBCBase}
\title{CoBC base method}
\usage{
coBCBase(y, learnerB, predB, N = 3, perc.full = 0.7, u = 100,
  max.iter = 50)
}
\arguments{
\item{y}{A vector with the labels of training instances. In this vector the 
unlabeled instances are specified with the value \code{NA}.}

\item{learnerB}{A function for training \code{N} supervised base classifiers.
This function needs two parameters, indexes and cls, where indexes indicates
the instances to use and cls specifies the classes of those instances.}

\item{predB}{A function for predicting the probabilities per classes.
This function must be two parameters, model and indexes, where the model
is a classifier trained with \code{learnerB} function and
indexes indicates the instances to predict.}

\item{N}{The number of classifiers used as committee members. All these classifiers 
are trained using the \code{learnerB} function. Default is 3.}

\item{perc.full}{A number between 0 and 1. If the percentage 
of new labeled examples reaches this value the self-labeling process is stopped.
Default is 0.7.}

\item{u}{Number of unlabeled instances in the pool. Default is 100.}

\item{max.iter}{Maximum number of iterations to execute in the self-labeling process. 
Default is 50.}
}
\value{
A list object of class "coBCBase" containing:
\describe{
  \item{model}{The final three base classifiers trained using the enlarged labeled set.}
  \item{instances.index}{The indexes of the total of training instances used to 
  train the three models. These indexes include the initial labeled instances
  and the newly labeled instances.
  These indexes are relative to the \code{y} argument.}
  \item{model.index}{List of three vectors of indexes related to the training instances 
  used per each classifier. These indexes are relative to \code{instances.index}.}
  \item{classes}{The levels of \code{y} factor.}
}
}
\description{
CoBC is a semi-supervised learning algorithm with a co-training 
style. This algorithm trains \code{N} classifiers with the learning scheme defined in 
\code{learnerB} using a reduced set of labeled examples. For each iteration, an unlabeled 
example is labeled for a classifier if the most confident classifications assigned by the 
other \code{N-1} classifiers agree on the labeling proposed. The unlabeled examples 
candidates are selected randomly from a pool of size \code{u}.
}
\details{
coBCBase can be helpful in those cases where the method selected as 
base classifier needs a \code{learner} and \code{pred} functions with other
specifications. For more information about the general coBC method,
please see \code{\link{coBC}} function. Essentially, \code{coBC}
function is a wrapper of \code{coBCBase} function.
}
\examples{
library(ssc)

## Load Wine data set
data(wine)

cls <- which(colnames(wine) == "Wine")
x <- wine[, -cls] # instances without classes
y <- wine[, cls] # the classes
x <- scale(x) # scale the attributes

## Prepare data
set.seed(20)
# Use 50\% of instances for training
tra.idx <- sample(x = length(y), size = ceiling(length(y) * 0.5))
xtrain <- x[tra.idx,] # training instances
ytrain <- y[tra.idx]  # classes of training instances
# Use 70\% of train instances as unlabeled set
tra.na.idx <- sample(x = length(tra.idx), size = ceiling(length(tra.idx) * 0.7))
ytrain[tra.na.idx] <- NA # remove class information of unlabeled instances

# Use the other 50\% of instances for inductive testing
tst.idx <- setdiff(1:length(y), tra.idx)
xitest <- x[tst.idx,] # testing instances
yitest <- y[tst.idx] # classes of testing instances

## Example: Training from a set of instances with 1-NN (knn3) as base classifier.
learnerB1 <- function(indexes, cls) 
  caret::knn3(x = xtrain[indexes, ], y = cls, k = 1)
predB1 <- function(model, indexes)  
  predict(model, xtrain[indexes, ]) 

set.seed(1)
md1 <- coBCBase(y = ytrain, learnerB1, predB1)

# Predict probabilities per instances using each model
h.prob <- lapply(
  X = md1$model, 
  FUN = function(m) predict(m, xitest)
)
# Combine probability matrices
prob <- coBCCombine(h.prob, ninstances = nrow(xitest), md1$classes)
# Get class per instance
cls1 <- md1$classes[apply(prob, 1, which.max)]
caret::confusionMatrix(table(cls1, yitest))

## Example: Training from a distance matrix with 1-NN (oneNN) as base classifier.
dtrain <- as.matrix(proxy::dist(x = xtrain, method = "euclidean", by_rows = TRUE))
learnerB2 <- function(indexes, cls) {
  m <- ssc::oneNN(y = cls)
  attr(m, "tra.idxs") <- indexes
  m
}

predB2 <- function(model, indexes)  {
  tra.idxs <- attr(model, "tra.idxs")
  d <- dtrain[indexes, tra.idxs]
  prob <- predict(model, d, type = "prob",  initial.value = 0) 
  prob
}

set.seed(1)
md2 <- coBCBase(y = ytrain, learnerB2, predB2)

# Predict probabilities per instances using each model
ditest <- proxy::dist(x = xitest, y = xtrain[md2$instances.index,],
                      method = "euclidean", by_rows = TRUE)

h.prob <- list()
ninstances <- nrow(dtrain)
for(i in 1:length(md2$model)){
  m <- md2$model[[i]]
  D <- ditest[, md2$model.index[[i]]]
  h.prob[[i]] <- predict(m, D, type = "prob",  initial.value = 0)
}
# Combine probability matrices
prob <- coBCCombine(h.prob, ninstances, md2$classes)
# Get class per instance
cls2 <- md2$classes[apply(prob, 1, which.max)]
caret::confusionMatrix(table(cls2, yitest))

}
